<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="WEEK 1、2、3 本文为个人笔记，只记了重要内容，不适合新手入手 线性回归 样本$(x^{(i)},y^{(i)})，i \in 1,2,…,m$ $x^{(i)}&#x3D;(x_1^{(i)},x_2^{(i)},…,x_n^{(i)})$ ，假设 $x^{(i)}$ 具有 $n$ 个特征 假想函数（目标函数）：h_\theta(x^{(i)})&#x3D;\theta_0+\theta_1x_1^{(i)}">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门">
<meta property="og:url" content="https://www.lzphi.cn/2018/02/01/MLWEEK123/index.html">
<meta property="og:site_name" content="LuoZui&#96; Blog">
<meta property="og:description" content="WEEK 1、2、3 本文为个人笔记，只记了重要内容，不适合新手入手 线性回归 样本$(x^{(i)},y^{(i)})，i \in 1,2,…,m$ $x^{(i)}&#x3D;(x_1^{(i)},x_2^{(i)},…,x_n^{(i)})$ ，假设 $x^{(i)}$ 具有 $n$ 个特征 假想函数（目标函数）：h_\theta(x^{(i)})&#x3D;\theta_0+\theta_1x_1^{(i)}">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2018-02-01T11:47:49.000Z">
<meta property="article:modified_time" content="2020-12-02T16:24:16.621Z">
<meta property="article:author" content="LuoZui">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="线性回归">
<meta property="article:tag" content="逻辑回归">
<meta property="article:tag" content="梯度下降法">
<meta name="twitter:card" content="summary">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
      
<link rel="stylesheet" href="/css/rtl.css">

    
    <!-- rss -->
    
    
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body class="max-width mx-auto px3 ltr">    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">简介</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="//github.com/luozui">项目</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2018/10/16/%E6%A8%A1%E6%9D%BF%E6%95%B4%E7%90%86/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2017/09/12/mac/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://www.lzphi.cn/2018/02/01/MLWEEK123/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&text=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&is_video=false&description=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门&body=Check out this article: https://www.lzphi.cn/2018/02/01/MLWEEK123/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&name=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.1.</span> <span class="toc-text">梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.1.1.</span> <span class="toc-text">特征优化：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.2.</span> <span class="toc-text">传统方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.3.</span> <span class="toc-text">code</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Octave</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.1.</span> <span class="toc-text">Ubuntu Install:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.2.</span> <span class="toc-text">Octave 入门:</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">逻辑回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">高级优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">4.0.1.</span> <span class="toc-text">调用 Octave 优化算法 example :</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">正则化项</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">5.1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.1.1.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.1.2.</span> <span class="toc-text">常规方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">5.2.</span> <span class="toc-text">逻辑回归</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">LuoZui` Blog</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2018-02-01T11:47:49.000Z" itemprop="datePublished">2018-02-01</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a>, <a class="tag-link-link" href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/" rel="tag">梯度下降法</a>, <a class="tag-link-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a>, <a class="tag-link-link" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" rel="tag">逻辑回归</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>WEEK 1、2、3</p>
<p><em>本文为个人笔记，只记了重要内容，不适合新手入手</em></p>
<h1><span id="线性回归">线性回归</span></h1><ul>
<li>样本$(x^{(i)},y^{(i)})，i \in 1,2,…,m$</li>
<li>$x^{(i)}=(x_1^{(i)},x_2^{(i)},…,x_n^{(i)})$ ，假设 $x^{(i)}$ 具有 $n$ 个特征</li>
<li>假想函数（目标函数）：<script type="math/tex; mode=display">h_\theta(x^{(i)})=\theta_0+\theta_1x_1^{(i)}+\theta_2x_2^{(i)}+…+\theta_n x_n^{(i)}</script></li>
<li>$h_\theta(x^{(i)})$ 表达式视具体情况而定</li>
<li><code>线性回归</code> 中 <code>线性</code> 的含义是参数 $\theta_j$ 都是一次的，而非 $h_\theta$ 的自变量，<code>回归</code> 应该是代价函数 $J$ 的自变量的回归</li>
<li>$\theta=(\theta_0,\theta_1,\theta_2,…,\theta_n)$ 称为参数，Machine Learning 的目的就是求出参数的合适取值使得 $h_\theta(x^{(i)})$ 更能体现 $x^{(i)} \to y^{(i)}$ 的映射关系</li>
<li>为此我们提出了一个衡量参数取值好坏的函数——代价函数：  <script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}{(h_\theta(x^{(i)}) - y^{(i)})^2}</script></li>
<li>现在问题转变为了求使得代价函数 $J(\theta)$ 最小时的参数 $\theta$，下面给出两种方法：</li>
</ul>
<h2><span id="梯度下降法">梯度下降法</span></h2><p><em>时间复杂度 $O(kn^2)$ ，适合当 n &gt; 10000 时 （k 为学习步数）</em></p>
<script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac {\partial J(\theta)}{\partial \theta_j}</script><ul>
<li>$\alpha$ 称学习速率（或步长），取值视情况而定，取值过大会导致不收敛;若当取值适当，在趋近极值时 $\frac {\partial J(\theta)}{\partial \theta_j}$ 也会变小，所以此时梯度下降法是收敛的，所以没必要担心 $\alpha$ 在趋近极值点的时候不改变导致无法收敛。</li>
<li>注意要先求出所有的 $temp_j= \theta_j - \alpha \frac {\partial J(\theta)}{\partial \theta_j}$ ，再对所有的 $\theta_j = temp_j$</li>
</ul>
<p>若取 $x_0^{(i)}=1$ ,求偏导后可以写成：</p>
<script type="math/tex; mode=display">\theta_j := \theta_j - \frac{\alpha}{m}\sum_{i=1}^{m}{[(h_\theta(x^{(i)}) - y^{(i)})\cdot x_j^{(i)}]}</script><p>用矩阵来表述的话：</p>
<script type="math/tex; mode=display">\theta := \theta - \frac{\alpha}{m}[X^T \cdot (X \cdot \theta - y)]</script><ul>
<li>$\theta \in \mathbb{R}^{(n+1) \times 1},X \in \mathbb{R}^{m \times (n+1)},y \in \mathbb{R}^{m \times 1}$</li>
</ul>
<h3><span id="特征优化">特征优化：</span></h3><p>尽量让 $-1&lt;x_i&lt;1$</p>
<script type="math/tex; mode=display">x_i := \frac {x_i-\mu_i}{s_i}</script><ul>
<li>$\mu_i$ 为第 $i$ 个特征的平均取值，$s_i$ 为第 $i$ 个特征的极差 $(max - min)$</li>
</ul>
<h2><span id="传统方法">传统方法</span></h2><p><em>时间复杂度 $O(n^3)$ ，适合当 n &lt; 10000 时</em></p>
<p>直接令$ \frac {\partial J(\theta)}{\partial \theta_i}=0$，解得：</p>
<script type="math/tex; mode=display">\theta=(X^TX)^{-1}X^Ty</script><ul>
<li>$\theta \in \mathbb{R}^{(n+1) \times 1},X \in \mathbb{R}^{m \times (n+1)},y \in \mathbb{R}^{m \times 1}$</li>
</ul>
<h2><span id="code">code</span></h2><p>featureNormalize</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[X_norm, mu, sigma]</span> = <span class="title">featureNormalize</span><span class="params">(X)</span></span></span><br><span class="line">mu = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="built_in">size</span>(X, <span class="number">2</span>));</span><br><span class="line">sigma = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="built_in">size</span>(X, <span class="number">2</span>));</span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:<span class="built_in">size</span>(X, <span class="number">2</span>)</span><br><span class="line">	mu(iter) = <span class="built_in">mean</span>(X(:, iter));</span><br><span class="line">	sigma(iter) = std(X(:, iter));</span><br><span class="line">	X_norm(:, iter) = (X(:, iter) - mu(iter)) / sigma(iter);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>computeCostMulti</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">J</span> = <span class="title">computeCost</span><span class="params">(X, y, theta)</span></span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J = <span class="number">1</span> / (<span class="number">2</span> * m) * sum((X * theta - y) .^ <span class="number">2</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>gradientDescentMulti</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta, J_history]</span> = <span class="title">gradientDescentMulti</span><span class="params">(X, y, theta, alpha, num_iters)</span></span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">J_history = <span class="built_in">zeros</span>(num_iters, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line">	theta -= alpha / m * X&#x27; * (X * theta - y);   </span><br><span class="line">    J_history(iter) = computeCostMulti(X, y, theta);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>normalEqn</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[theta]</span> = <span class="title">normalEqn</span><span class="params">(X, y)</span></span></span><br><span class="line">theta = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X, <span class="number">2</span>), <span class="number">1</span>);</span><br><span class="line">theta = pinv(X&#x27; * X) * X&#x27; * y;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<hr>
<h1><span id="octave">Octave</span></h1><p><a target="_blank" rel="noopener" href="https://www.gnu.org/software/octave/">https://www.gnu.org/software/octave/</a></p>
<h2><span id="ubuntu-install">Ubuntu Install:</span></h2><pre><code>sudo apt-add-repository ppa:octave/stable
sudo apt-get update
sudo apt-get install octave
</code></pre><h2><span id="octave-入门">Octave 入门:</span></h2><p>内容比较多，推荐两篇文章</p>
<p><a target="_blank" rel="noopener" href="http://blog.csdn.net/weixin_36106941/article/details/64443944">http://blog.csdn.net/weixin_36106941/article/details/64443944</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/leezx/p/5635056.html">https://www.cnblogs.com/leezx/p/5635056.html</a></p>
<h1><span id="逻辑回归">逻辑回归</span></h1><p>线性回归是用来预测某个点的取值，逻辑回归是预测某个点具有某种特征的概率</p>
<p>为了达到我们的目的，重现建立模型：</p>
<script type="math/tex; mode=display">\begin{aligned}
h_\theta(x) & = g(\theta^Tx)\\\ \\\
g(z) & = \frac{1}{1+e^{-z}}\\\ \\\
J(\theta) &=\frac {1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)},y^{(i)}))\\\
 \\\
Cost(h_\theta(x,y)) &= 
\begin{cases}
&-log(h_\theta(x))     && y = 1 \\\ \\\
&-log(1 - h_\theta(x)) && y = 0
\end{cases}
\end{aligned}</script><p><em>Note: $y = 0$ or $1$ always , and <code>log</code> is <code>ln</code></em></p>
<p>可以写成：</p>
<script type="math/tex; mode=display">Cost(h_\theta(x,y)) = -[y\cdot log(h_\theta(x)) + (1 - y)\cdot log(1 - h_\theta(x)]</script><p>对代价函数求偏倒后发现和线性回归代价函数求偏倒的结果形式上是完全一样的:</p>
<script type="math/tex; mode=display">\frac {\partial J(\theta)}{\partial \theta_j}=\frac{1}{m}\sum_{i=1}^{m}{[(h_\theta(x^{(i)}) - y^{(i)})\cdot x_j^{(i)}]}</script><p>cost function code<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">costFunction</span><span class="params">(theta, X, y)</span></span></span><br><span class="line">m = <span class="built_in">length</span>(y); <span class="comment">% number of training examples</span></span><br><span class="line">tmp = sigmoid(X*theta);</span><br><span class="line">J = <span class="number">-1</span> / m * (y&#x27;*<span class="built_in">log</span>(tmp)+(<span class="number">1</span>-y)&#x27;*<span class="built_in">log</span>(<span class="number">1</span>-tmp));</span><br><span class="line">grad = <span class="number">1</span> / m * X&#x27; * (sigmoid(X * theta) - y);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<h1><span id="高级优化算法">高级优化算法</span></h1><p>Optimization algorithms:</p>
<ul>
<li>Gradient descent</li>
<li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<p>Advantages:</p>
<ul>
<li>No need to manually pick $\alpha$</li>
<li>Often faster than grdicent descent.</li>
</ul>
<p>disadvantages:</p>
<ul>
<li>More complex</li>
</ul>
<h3><span id="调用-octave-优化算法-example">调用 Octave 优化算法 example :</span></h3><p>[first] 定义 cost function:</p>
<p>costFunction.m<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[jval, gradient]</span> = <span class="title">costFunction</span><span class="params">(theta, X, y)</span></span></span><br><span class="line">	<span class="comment">% jval := J(theta)</span></span><br><span class="line">    <span class="comment">% gradient := grad J(theta)</span></span><br></pre></td></tr></table></figure><br>[then] 键入命令</p>
<pre><code>options = optimset(&#39;GrandObj&#39;, &#39;on&#39;, &#39;MaxIter&#39;, &#39;100&#39;);
initialTheta = zeros(n + 1, 1)
[optTheta, functionVal, exitFlag] = fminunc(@(t)costFunction(t, X, y), initialTheta, options);
</code></pre><h1><span id="正则化项">正则化项</span></h1><h2><span id="线性回归">线性回归</span></h2><p>为了防止 overfitting（过度拟合），对 cost function 引入了正则化项 $\frac\lambda{2m}\sum_{i=1}^{n}\theta_i^2$</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{i=1}^{n}\theta_i^2]</script><p>求偏导：</p>
<script type="math/tex; mode=display">\frac {\partial J(\theta)}{\partial \theta_j} =
\begin{cases}
&\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}  &j=0\\\ \\\
&\frac{1}{m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+ \lambda\cdot \theta_j]&j>0
\end{cases}</script><p><strong>注意！！！ $\theta_0$ 不需要惩罚</strong></p>
<h3><span id="梯度下降法">梯度下降法</span></h3><p>Repeat {</p>
<script type="math/tex; mode=display">\begin{aligned}
\theta_0 & := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\\
\theta_j & := \theta_j - \alpha\frac{1}{m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+ \lambda\cdot \theta_j]
\end{aligned}</script><p>}</p>
<p>对上整理后：</p>
<script type="math/tex; mode=display">\theta_j  := \theta_j(1- \alpha\frac\lambda m) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><p>由于 $1- \alpha\frac\lambda m&lt;1$ ，可以将 $\theta_j(1- \alpha\frac\lambda m)$ 写成 $\theta_j\cdot 0.99$</p>
<h3><span id="常规方法">常规方法</span></h3><p>直接令$ \frac {\partial J(\theta)}{\partial \theta_i}=0$，解得：</p>
<script type="math/tex; mode=display">\theta=(X^TX + \lambda \cdot \underbrace {\begin{bmatrix} 0 & 0 & 0 &\cdots &0 \\\ 0 & 1 & 0 &\cdots &0 \\\ 0 & 0 & 1 &\cdots &0 \\\ \vdots & \vdots & \vdots & \ddots & \vdots \\\ 0&0&0&\cdots &1  \end{bmatrix}}_{(n+1)\times (n+1)})^{-1}X^Ty</script><ul>
<li>Suppose $m \le n$ (m: examples  , n : features)</li>
<li>在 $\lambda&gt;0$ 时，括号内的矩阵总是可逆的</li>
<li>$\theta \in \mathbb{R}^{(n+1) \times 1},X \in \mathbb{R}^{m \times (n+1)},y \in \mathbb{R}^{m \times 1}$</li>
</ul>
<h2><span id="逻辑回归">逻辑回归</span></h2><p>对 cost function 引入了正则化项 $\frac\lambda{2m}\sum_{i=1}^{n}\theta_i^2$</p>
<script type="math/tex; mode=display">\begin{aligned}
J(\theta) &=\frac {1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)},y^{(i)}))+\frac\lambda{2m}\sum_{i=1}^{n}\theta_i^2 \\\
&=-\frac {1}{m}\sum_{i=1}^{m}[y^{(i)}\cdot log(h_\theta(x^{(i)})) + (1 - y^{(i)})\cdot log(1 - h_\theta(x^{(i)})]+\frac\lambda{2m}\sum_{i=1}^{n}\theta_i^2
\end{aligned}</script><p>求偏导数后结果形式和线性回归是一样的：</p>
<script type="math/tex; mode=display">\frac {\partial J(\theta)}{\partial \theta_j} = 
\begin{cases}
&\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}  &j=0\\\ \\\
&\frac{1}{m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+ \lambda\cdot \theta_j]&j>0
\end{cases}</script><p><strong>注意！！！ $\theta_0$ 不需要惩罚</strong></p>
<p>cost function code<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[J, grad]</span> = <span class="title">costFunctionReg</span><span class="params">(theta, X, y, lambda)</span></span></span><br><span class="line">tmp = sigmoid(X*theta);</span><br><span class="line">J = <span class="number">-1</span> / m * (y&#x27;*<span class="built_in">log</span>(tmp)+(<span class="number">1</span>-y)&#x27;*<span class="built_in">log</span>(<span class="number">1</span>-tmp)) + lambda / (<span class="number">2</span> * m) * sum(theta(<span class="number">2</span>:<span class="built_in">size</span>(theta, <span class="number">1</span>),<span class="number">1</span>) .^ <span class="number">2</span>);</span><br><span class="line">theta(<span class="number">1</span>) = <span class="number">0</span>;</span><br><span class="line">grad = <span class="number">1</span> / m * (X&#x27; * (tmp - y) + lambda * theta);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<hr>
<p><a target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a><br>教学方：  Andrew Ng, Co-founder, Coursera; Adjunct Professor, Stanford University; formerly head of Baidu AI Group/Google Brain</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">简介</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="//github.com/luozui">项目</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.1.</span> <span class="toc-text">梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">1.1.1.</span> <span class="toc-text">特征优化：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.2.</span> <span class="toc-text">传统方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">1.3.</span> <span class="toc-text">code</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Octave</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.1.</span> <span class="toc-text">Ubuntu Install:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">2.2.</span> <span class="toc-text">Octave 入门:</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">逻辑回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">高级优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">4.0.1.</span> <span class="toc-text">调用 Octave 优化算法 example :</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">正则化项</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">5.1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.1.1.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">5.1.2.</span> <span class="toc-text">常规方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">5.2.</span> <span class="toc-text">逻辑回归</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://www.lzphi.cn/2018/02/01/MLWEEK123/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&text=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&is_video=false&description=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门&body=Check out this article: https://www.lzphi.cn/2018/02/01/MLWEEK123/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&title=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://www.lzphi.cn/2018/02/01/MLWEEK123/&name=Machine Learning - WEEK 1 2 3- 线性回归 、逻辑回归、梯度下降法及其优化算法、Octave 入门&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 LuoZui
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">简介</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a target="_blank" rel="noopener" href="//github.com/luozui">项目</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
<!-- styles -->

<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">


<!-- jquery -->

<script src="/lib/jquery/jquery.min.js"></script>


<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->





